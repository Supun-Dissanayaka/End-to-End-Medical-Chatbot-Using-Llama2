{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"yt5jitvi7wnM"},"outputs":[],"source":["from langchain import PromptTemplate\n","from langchain.chains import RetrievalQA\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Pinecone\n","import pinecone\n","from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.prompts import PromptTemplate\n","from langchain.llms import  ctransformers\n","import os\n","import numpy as np\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"-smnlGur7wnV"},"source":["## **Extract data from the PDF**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Qdmh6My-7wnX"},"outputs":[],"source":["def load_pdf(data):\n","    loader=DirectoryLoader(data,\n","                    glob=\"*.pdf\",\n","                    loader_cls=PyPDFLoader)\n","\n","    documents=loader.load()\n","\n","    return documents"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"a5MEzCvl7wnY"},"outputs":[],"source":["extracted_data = load_pdf(\"/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["extracted_data"]},{"cell_type":"markdown","metadata":{},"source":["## **Text preprocessing**"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/supunlakshan/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     /home/supunlakshan/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize\n","\n","# Download necessary resources\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","# Define preprocessing functions\n","def clean_text(text):\n","    # Remove non-alphanumeric characters but keep URLs intact\n","    text = re.sub(r'[^\\w\\s\\./-]', '', text)\n","    text = ' '.join(text.split())\n","    return text\n","\n","def lowercase_text(text):\n","    # Convert text to lowercase\n","    return text.lower()\n","\n","def remove_stopwords(text):\n","    # Remove stopwords from the text (excluding URLs)\n","    stop_words = set(stopwords.words('english'))\n","    tokens = text.split()\n","    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and not re.match(r'^https?://', token)]\n","    return ' '.join(filtered_tokens)\n","\n","# Apply preprocessing to each document\n","preprocessed_documents = []\n","for document in extracted_data:\n","    content = document.page_content\n","    # Clean text (keeping URLs)\n","    cleaned_content = clean_text(content)\n","    # Lowercase text\n","    lowercase_content = lowercase_text(cleaned_content)\n","    # Remove stopwords (excluding URLs)\n","    filtered_content = remove_stopwords(lowercase_content)\n","    # Update the 'page_content' attribute of the document with preprocessed text\n","    document.page_content = filtered_content\n","    # Append the preprocessed document to the list\n","    preprocessed_documents.append(document)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["637"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(preprocessed_documents)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessed_documents"]},{"cell_type":"markdown","metadata":{"id":"ypSUv10M7wna"},"source":["## **Create text chunks**"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"-hPbINrV7wna"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of text_chunks: 4564\n"]}],"source":["def text_split(preprocessed_documents):\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n","    text_chunks = text_splitter.split_documents(preprocessed_documents)  # Wrap extracted_data in a list\n","    return text_chunks\n","\n","text_chunks=text_split(list(preprocessed_documents))\n","print(\"length of text_chunks:\",len(text_chunks))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# text_chunks"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of text_chunks: 4564\n","<class 'list'>\n"]}],"source":["\n","print(\"Length of text_chunks:\", len(text_chunks))\n","print(type(text_chunks))"]},{"cell_type":"markdown","metadata":{"id":"N4tqmCCu7wnd"},"source":["## **Embedding**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/supunlakshan/anaconda3/envs/mchatbot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/home/supunlakshan/anaconda3/envs/mchatbot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[[ 0.00174608 -0.03350287 -0.03290391 ... -0.00555645  0.10660913\n","   0.05099721]\n"," [-0.00524566 -0.01224833 -0.03722573 ...  0.0075348   0.08098163\n","   0.05183331]\n"," [ 0.02046325 -0.0224895  -0.00601478 ... -0.0070935  -0.00974825\n","   0.04680153]\n"," ...\n"," [-0.09775922  0.06158298 -0.04184233 ...  0.05067963 -0.04601221\n","  -0.07389956]\n"," [ 0.03905462  0.01553802 -0.0352585  ... -0.0279488  -0.05134996\n","   0.12449328]\n"," [ 0.01676182  0.05766533 -0.09481081 ... -0.04915264  0.03086303\n","   0.09051288]]\n"]}],"source":["\n","from sentence_transformers import SentenceTransformer\n","\n","# Extract text from Document objects\n","text_list = [t.page_content for t in text_chunks]\n","\n","# Initialize SentenceTransformer model\n","model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","\n","# Encode the text chunks\n","embeddings = model.encode(text_list)\n","\n","# Print the embeddings\n","print(embeddings)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Embedding dimensions: (4564, 384)\n","<class 'numpy.ndarray'>\n","4564\n"]}],"source":["# Get the dimensions of the embeddings\n","embedding_dimensions = embeddings.shape\n","\n","# Print the dimensions\n","print(\"Embedding dimensions:\", embedding_dimensions)\n","\n","print(type(embeddings))\n","print(len(embeddings))"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"XqIeVGNr7wni"},"outputs":[],"source":["from pinecone import Pinecone\n","\n","pc = Pinecone(api_key=\"afedfe83-ec30-4c3c-b51c-2e565474af4a\")\n","index = pc.Index(\"mymchatbot\")"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"JHkXXWXbAoZ5"},"outputs":[],"source":["# Specify your namespace\n","namespace = \"book1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTLXe8IW7wni"},"outputs":[],"source":["import uuid\n","ids = [str(uuid.uuid4()) for _ in range(len(embeddings))]\n","\n","print(len(ids))\n","print(ids)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["vectors_to_upsert = [\n","    {\n","        \"id\": str(ids[i]),  # Ensure each ID is a string\n","        \"values\": embeddings[i],  # The embedding vector for the text chunk\n","        \"metadata\": {\"page_content\": str(text_chunks[i])}  # Storing page content as metadata\n","    } for i in range(len(embeddings))\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(type(vectors_to_upsert))\n","print(vectors_to_upsert[1000])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Proceed with the upsert\n","def upsert_in_batches(index, vectors, namespace, batch_size=100):\n","    for i in range(0, len(vectors), batch_size):\n","        batch = vectors[i:i+batch_size]\n","        index.upsert(vectors=batch, namespace=namespace)\n","\n","# Usage\n","upsert_in_batches(index, vectors_to_upsert, namespace, batch_size=100)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":406,"status":"ok","timestamp":1714465348830,"user":{"displayName":"Supun Lakshan","userId":"07776099517598475771"},"user_tz":-330},"id":"ceEBeqwyGRAg","outputId":"fed5f340-4f56-45df-cc8a-0e533e67c16a"},"outputs":[{"data":{"text/plain":["{'dimension': 384,\n"," 'index_fullness': 0.0,\n"," 'namespaces': {'book1': {'vector_count': 4564}},\n"," 'total_vector_count': 4564}"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["index.describe_index_stats()"]},{"cell_type":"markdown","metadata":{"id":"gucdAgYKFCUJ"},"source":["## **If we already have an index we can load it like this**"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["query=(\"what is the Autism?\")"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["query_vector = model.encode(query)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(query_vector.shape)\n","print(len(query_vector))\n","query_vector"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"elapsed":412,"status":"error","timestamp":1714468338117,"user":{"displayName":"Supun Lakshan","userId":"07776099517598475771"},"user_tz":-330},"id":"ZJkRVya-KcuQ","outputId":"6709eae7-ed83-44ad-bf91-d62db403340f"},"outputs":[],"source":["# Assuming `query_vector` is a numpy array\n","query_vector_list = query_vector.tolist()  # Convert numpy array to a list\n","\n","query_result = index.query(\n","    vector=query_vector_list,  # Your query vector\n","    namespace=\"book1\",\n","    top_k=5,  # Number of top similar vectors to retrieve\n","    include_metadata=True  # Ensure metadata is included in the response\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["page_content='autism research naar. http//www. naar.org. national information center children youth dis- abilities. http//www.nichcy.org/transitn.htm. carol a. turkington autograft seeskin grafting gale encyclopedia medicine 2 421autismgem - 0001 0432 - 10/22/03 143 pm page 421' metadata={'source': '/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data/Medical_book.pdf', 'page': 434}\n","page_content='times often boys usual-ly first-born occurs around world racesand social backgrounds. autism usually evident thefirst three years life although children itshard tell problem develops. sometimes thecondition isnt diagnosed child enters school. person autism symptoms ranging mild severe 10 extraor-dinary ability one area mathematics mem-ory music art. children known autisticsavants formerly known idiot savants.. causes symptoms autism brain disorder affects way brain uses transmits information.' metadata={'source': '/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data/Medical_book.pdf', 'page': 431}\n","page_content='disorders. also important rule outother problems seem similar autism. gale encyclopedia medicine 2 419autism autistic child encouraged interact guinea pig effort improve social interaction. helen b. senisi. photo researchers inc. reproduced permission.gem - 0001 0432 - 10/22/03 143 pm page 419' metadata={'source': '/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data/Medical_book.pdf', 'page': 432}\n","page_content='guage together ritualistic compulsive behavior bizarre responses environment. description autism lifelong disorder interferes ability understand seen heard touched.this cause profound problems personal behaviorand ability relate others. person withautism must learn communicate normally andhow relate people objects events. howevernot patients suffer degree impairment.there full spectrum symptoms rangefrom mild severe. autism occurs many one two per 1000 children. found four times often boys' metadata={'source': '/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data/Medical_book.pdf', 'page': 431}\n","page_content='autistic children seem overwhelmed ownsenses. child autism may ignore objects gale encyclopedia medicine 2 418autismgem - 0001 0432 - 10/22/03 143 pm page 418' metadata={'source': '/media/supunlakshan/Learning Hub/AI & Machine Learning/LLM,OpenAI/End-to-End-Medical-Chatbot/data/Medical_book.pdf', 'page': 431}\n"]}],"source":["# for match in query_result['matches']:\n","#     print(f\"ID: {match['id']}, Score: {match['score']}, Page Content: {match['metadata'].get('page_content')}\")\n","\n","for match in query_result['matches']:\n","    print((f\"{match['metadata'].get('page_content')}\"))\n"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714468927422,"user":{"displayName":"Supun Lakshan","userId":"07776099517598475771"},"user_tz":-330},"id":"fDaevscq7wnj"},"outputs":[],"source":["prompt_template=\"\"\"\n","Use the following pieces of information to answer the user's question.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","Context: {context}\n","Question: {question}\n","\n","Only return the helpful answer below and nothing else.\n","Helpful answer:\n","\"\"\""]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1714468931824,"user":{"displayName":"Supun Lakshan","userId":"07776099517598475771"},"user_tz":-330},"id":"U2SqbYYm7wnj"},"outputs":[],"source":["PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n","chain_type_kwargs={\"prompt\": PROMPT}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":508,"status":"error","timestamp":1714468954713,"user":{"displayName":"Supun Lakshan","userId":"07776099517598475771"},"user_tz":-330},"id":"uF9Xsl_t7wnk","outputId":"31adebf2-45e3-470f-f725-fbd641c3ec9f"},"outputs":[],"source":["llm = ctransformers.CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n","                                  model_type=\"llama\",\n","                                  config={'max_new_tokens': 512,\n","                                          'temperature': 0.8}\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/supunlakshan/anaconda3/envs/mchatbot/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["from langchain_pinecone import PineconeVectorStore\n","\n","# Assuming this is your sentence transformer model\n","model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","\n","# Create the PineconeVectorStore instance (assuming you have the index details)\n","vectorstore = PineconeVectorStore(index, model)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["from langchain.retrievers import SelfQueryRetriever\n","\n","document_content_description = \"page_content\"\n","metadata_field_info = {}  # Define the metadata_field_info variable (if needed)\n","\n","retriever = SelfQueryRetriever.from_llm(\n","    llm,\n","    vectorstore,\n","    document_content_description,\n","    metadata_field_info,\n","    enable_limit=True,\n","    verbose=True,\n",")\n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",  # Replace \"stuff\" with your actual chain type name\n","    retriever=retriever,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk  # Import for potential text truncation\n","\n","def get_user_input():\n","  \"\"\"Prompts the user for input and handles potential truncation.\"\"\"\n","  max_tokens = 512  # Adjust this limit as needed\n","  while True:\n","    user_input = input(f\"Input Prompt:\")\n","    if len(nltk.word_tokenize(user_input)) <= max_tokens:  # Check token length\n","      return user_input\n","    else:\n","      print(f\"Warning: Input exceeds maximum length ({max_tokens} tokens). Please try again with a shorter prompt.\")\n","\n","while True:\n","  user_input = get_user_input()  # Get user input with truncation handling\n","\n","  # Embed the user query using encode\n","  query_vector = model.encode(user_input)\n","\n","  try:\n","    # Use Langchain for retrieval and question answering (using invoke)\n","    result = qa.invoke({\"query\": query_vector.tolist()})\n","    print(\"Response :\", result[\"result\"])\n","  except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","    print(\"Please try again with a different prompt.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
